{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feb34c2a-d413-47b1-bc85-e132a469a8c2",
   "metadata": {},
   "source": [
    "# **Dataset preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "395ad00d-ffda-431a-9861-3d8273f88a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov 22 09:47:17 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4050 ...    Off | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   42C    P0              N/A /  80W |      8MiB /  6141MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1933      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5afeb4-0a6c-4e44-8c13-17e40dab3733",
   "metadata": {},
   "source": [
    "## Install necessary librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e36faf53-cf63-4c9e-a0cd-e90b7e495d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy==1.24.3 in /home/tom/.local/lib/python3.10/site-packages (1.24.3)\n",
      "Requirement already satisfied: opencv-python in /home/tom/.local/lib/python3.10/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: tensorflow in /home/tom/.local/lib/python3.10/site-packages (2.17.1)\n",
      "Requirement already satisfied: pandas in /home/tom/.local/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /home/tom/.local/lib/python3.10/site-packages (1.5.2)\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.18.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /home/tom/.local/lib/python3.10/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/tom/.local/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/tom/.local/lib/python3.10/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/tom/.local/lib/python3.10/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/tom/.local/lib/python3.10/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /home/tom/.local/lib/python3.10/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/tom/.local/lib/python3.10/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/tom/.local/lib/python3.10/site-packages (from tensorflow) (4.25.5)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/tom/.local/lib/python3.10/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: packaging in /home/tom/.local/lib/python3.10/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/tom/.local/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/tom/.local/lib/python3.10/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/tom/.local/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /home/tom/.local/lib/python3.10/site-packages (from tensorflow) (2.17.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/tom/.local/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/tom/.local/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (59.6.0)\n",
      "Requirement already satisfied: keras>=3.2.0 in /home/tom/.local/lib/python3.10/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/tom/.local/lib/python3.10/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/tom/.local/lib/python3.10/site-packages (from tensorflow) (1.66.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/tom/.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/tom/.local/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/tom/.local/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/tom/.local/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/lib/python3/dist-packages (from scikit-learn) (1.8.0)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting sentry-sdk>=2.0.0\n",
      "  Downloading sentry_sdk-2.19.0-py2.py3-none-any.whl (322 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 KB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from wandb) (5.4.1)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /home/tom/.local/lib/python3.10/site-packages (from wandb) (6.0.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/lib/python3/dist-packages (from wandb) (8.0.3)\n",
      "Requirement already satisfied: platformdirs in /home/tom/.local/lib/python3.10/site-packages (from wandb) (4.3.6)\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.3.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: namex in /home/tom/.local/lib/python3.10/site-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: rich in /home/tom/.local/lib/python3.10/site-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: optree in /home/tom/.local/lib/python3.10/site-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow) (1.26.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/tom/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.3/126.3 KB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /home/tom/.local/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/tom/.local/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/tom/.local/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/tom/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/tom/.local/lib/python3.10/site-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/tom/.local/lib/python3.10/site-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/tom/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
      "Installing collected packages: urllib3, smmap, setproctitle, docker-pycreds, sentry-sdk, gitdb, gitpython, wandb\n",
      "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.19.0 setproctitle-1.3.4 smmap-5.0.1 urllib3-2.2.3 wandb-0.18.7\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.24.3 opencv-python tensorflow pandas scikit-learn wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11b22e3-3371-4e82-9c72-15efe660f3af",
   "metadata": {},
   "source": [
    "## Load images and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb4a3883-c7a3-438f-9765-d3e39943ea43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def load_training_data(image_dir, mask_dir):\n",
    "    \"\"\"\n",
    "    Loads training images and their corresponding masks.\n",
    "    Args:\n",
    "        image_dir (str): Path to the directory containing training images.\n",
    "        mask_dir (str): Path to the directory containing ground truth masks.\n",
    "    Returns:\n",
    "        list of tuples: [(image, mask), ...]\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    image_filenames = sorted(os.listdir(image_dir))\n",
    "    mask_filenames = sorted(os.listdir(mask_dir))\n",
    "\n",
    "    for image_file, mask_file in zip(image_filenames, mask_filenames):\n",
    "        # Load the image and mask\n",
    "        image_path = os.path.join(image_dir, image_file)\n",
    "        mask_path = os.path.join(mask_dir, mask_file)\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # Load mask as grayscale\n",
    "\n",
    "        if image is None or mask is None:\n",
    "            raise FileNotFoundError(f\"Missing image or mask: {image_path}, {mask_path}\")\n",
    "\n",
    "        data.append((image, mask))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9510899-073f-485a-9821-3ec1e40b42e9",
   "metadata": {},
   "source": [
    "## Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60eca25f-36f9-4c56-9af2-59edb57188fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_training_data(\"data/train\", \"data/train_gt\")\n",
    "\n",
    "# Display the first image and mask\n",
    "image, mask = data[0]\n",
    "cv2.imshow(\"Image\", image)\n",
    "cv2.imshow(\"Mask\", mask)  # Scale mask for display if needed\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec79215-6598-472c-903a-3cc5a9bdb2cf",
   "metadata": {},
   "source": [
    "## Reprocess images and masks\n",
    "\n",
    "Resize images and normalize their pixel values for input to the model. Masks will also be resized, with their values remaining binary (0 and 1 for segmentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54181725-5d6f-4325-90c7-32374e8b100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image, mask, size=(256, 256)):\n",
    "    \"\"\"\n",
    "    Resizes and normalizes the image and mask.\n",
    "    Args:\n",
    "        image (np.ndarray): Original image.\n",
    "        mask (np.ndarray): Original mask.\n",
    "        size (tuple): Target size (height, width).\n",
    "    Returns:\n",
    "        tuple: Preprocessed image and mask.\n",
    "    \"\"\"\n",
    "    # Resize image and mask\n",
    "    image = cv2.resize(image, size) / 255.0  # Normalize image to [0, 1]\n",
    "    mask = cv2.resize(mask, size, interpolation=cv2.INTER_NEAREST)  # Resize mask\n",
    "    mask = (mask > 0).astype(np.uint8)  # Ensure binary mask\n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8807a81e-fd8b-4354-b2e6-a3bd0b81c8c8",
   "metadata": {},
   "source": [
    "Example usage :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5678d360-8b46-433d-a75b-710f5649bd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (256, 256, 3)\n",
      "Mask shape: (256, 256)\n"
     ]
    }
   ],
   "source": [
    "preprocessed_image, preprocessed_mask = preprocess(data[0][0], data[0][1])\n",
    "print(\"Image shape:\", preprocessed_image.shape)  # Example: (256, 256, 3)\n",
    "print(\"Mask shape:\", preprocessed_mask.shape)    # Example: (256, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa46c8f-d3f7-447f-952c-10ae3a617459",
   "metadata": {},
   "source": [
    "## Prepare training and validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "add9ee92-5615-4cc4-a382-61ba453474ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_dataset(data, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Splits data into training and validation sets.\n",
    "    Args:\n",
    "        data (list): List of (image, mask) pairs.\n",
    "        test_size (float): Fraction of the dataset for validation.\n",
    "    Returns:\n",
    "        tuple: (train_data, val_data)\n",
    "    \"\"\"\n",
    "    return train_test_split(data, test_size=test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c76c63d-37e8-45e4-94f2-b715d17eb773",
   "metadata": {},
   "source": [
    "Example usage :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e250bbc5-aef4-46e7-b9e3-a7b0371790b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 800\n",
      "Validation samples: 200\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data = split_dataset(data)\n",
    "print(\"Training samples:\", len(train_data))\n",
    "print(\"Validation samples:\", len(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e935add-968b-4830-af06-3f3d8c0183de",
   "metadata": {},
   "source": [
    "## Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56f210e3-a062-4959-96cd-8111052e0410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data, batch_size=16, size=(256, 256)):\n",
    "    \"\"\"\n",
    "    Generator for batching data.\n",
    "    Args:\n",
    "        data (list): List of (image, mask) pairs.\n",
    "        batch_size (int): Number of samples per batch.\n",
    "        size (tuple): Target size (H, W).\n",
    "    Yields:\n",
    "        tuple: (batch_images, batch_masks)\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        batch_images, batch_masks = [], []\n",
    "        for _ in range(batch_size):\n",
    "            image, mask = data[np.random.randint(len(data))]\n",
    "            image, mask = preprocess(image, mask, size)\n",
    "            batch_images.append(image)\n",
    "            batch_masks.append(mask)\n",
    "        yield np.array(batch_images), np.array(batch_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc90aac4-5315-4496-9dbb-f808735d644e",
   "metadata": {},
   "source": [
    "Example usage :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ebab94d-8479-4509-a863-052b8674d497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch image shape: (16, 256, 256, 3)\n",
      "Batch mask shape: (16, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "train_gen = data_generator(train_data)\n",
    "val_gen = data_generator(val_data)\n",
    "\n",
    "# Fetch one batch\n",
    "batch_images, batch_masks = next(train_gen)\n",
    "print(\"Batch image shape:\", batch_images.shape)  # (16, 256, 256, 3)\n",
    "print(\"Batch mask shape:\", batch_masks.shape)    # (16, 256, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d16a716-e48b-4e3b-b5c5-68b338910400",
   "metadata": {},
   "source": [
    "## Load test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc35366f-299a-456f-8364-f6c4d817c2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_images(image_dir):\n",
    "    \"\"\"\n",
    "    Loads test images for inference.\n",
    "    Args:\n",
    "        image_dir (str): Path to the test image directory.\n",
    "    Returns:\n",
    "        list: List of test images.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    for image_file in sorted(os.listdir(image_dir)):\n",
    "        image_path = os.path.join(image_dir, image_file)\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Test image {image_path} not found!\")\n",
    "        images.append(image)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc91a0f-3b1a-46c3-a9e2-f0441543b552",
   "metadata": {},
   "source": [
    "Example usage :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fda576fd-dd46-4b20-862c-bc3b6c2e733f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test images: 200\n"
     ]
    }
   ],
   "source": [
    "test_images = load_test_images(\"data/test\")\n",
    "print(\"Number of test images:\", len(test_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24704c70-673a-4991-8fe4-bafadb58fec0",
   "metadata": {},
   "source": [
    "# **Model implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0ab127-92c4-46f0-aec0-fff3c43ba6d0",
   "metadata": {},
   "source": [
    "## Define the model architecture (UNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a1fe2de-56fa-4b94-a771-745a686b8079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 15:32:57.393427: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-21 15:32:57.512970: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-21 15:32:57.558246: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-21 15:32:57.575576: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-21 15:32:57.676102: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-21 15:32:58.421708: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def unet_model(input_size=(256, 256, 3)):\n",
    "    inputs = layers.Input(input_size)\n",
    "    \n",
    "    # Encoder path (Downsampling)\n",
    "    conv1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = layers.MaxPooling2D((2, 2))(conv1)\n",
    "\n",
    "    conv2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = layers.MaxPooling2D((2, 2))(conv2)\n",
    "\n",
    "    conv3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    pool3 = layers.MaxPooling2D((2, 2))(conv3)\n",
    "\n",
    "    # Bottleneck\n",
    "    conv4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(pool3)\n",
    "    conv4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(conv4)\n",
    "\n",
    "    # Decoder path (Upsampling)\n",
    "    up5 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv4)\n",
    "    concat5 = layers.concatenate([up5, conv3], axis=3)\n",
    "    conv5 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(concat5)\n",
    "    conv5 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(conv5)\n",
    "\n",
    "    up6 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv5)\n",
    "    concat6 = layers.concatenate([up6, conv2], axis=3)\n",
    "    conv6 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(concat6)\n",
    "    conv6 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(conv6)\n",
    "\n",
    "    up7 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv6)\n",
    "    concat7 = layers.concatenate([up7, conv1], axis=3)\n",
    "    conv7 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(concat7)\n",
    "    conv7 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(conv7)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(conv7)\n",
    "\n",
    "    # Create model\n",
    "    model = models.Model(inputs, outputs)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a6502d-6237-43ff-a5be-a173b7162dce",
   "metadata": {},
   "source": [
    "## Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58d3ebb7-346f-4f2b-9b29-3aa21a8d1d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1732199640.655443   16754 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-21 15:34:00.757377: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "model = unet_model(input_size=(256, 256, 3))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='dice_loss', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d1d560-3f15-473c-a291-15ff02da63d1",
   "metadata": {},
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abff05f-47c5-46df-8afe-bf4b2a318202",
   "metadata": {},
   "source": [
    "## Loss function : Dice loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69bf2df2-4260-4301-a08d-0b3958209745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def dice_loss(y_true, y_pred, smooth=1e-6):\n",
    "    \n",
    "    # Calculate the intersection of the predicted and true masks\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    # Calculate the Dice coefficient loss\n",
    "    return 1 - (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec6b1a2-dfe5-4ef1-b16d-738c374411d4",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8b71883-868c-4526-ab4b-c59a107f613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0710c9-603c-426e-a4d9-1f72ea695196",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6ae4d66-9327-45f5-9661-0e88f8251cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "      5/Unknown \u001b[1m69s\u001b[0m 13s/step - accuracy: 0.6300 - loss: 0.6035"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Train the model using the data generators\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Training data generator\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Number of epochs, adjust as needed\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Validation data generator # Optional callbacks\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1567\u001b[0m   )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Train the model using the data generators\n",
    "history = model.fit(\n",
    "    train_gen,  # Training data generator\n",
    "    epochs=50,  # Number of epochs, adjust as needed\n",
    "    validation_data=val_gen,  # Validation data generator # Optional callbacks\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
